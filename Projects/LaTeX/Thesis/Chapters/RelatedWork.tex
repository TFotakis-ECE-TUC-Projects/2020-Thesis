\chapter{Related Work}

\label{Chapter-Related-Work}

\section{CNN Architectures}
\label{section:CNN-Architectures}
The essential part of every machine learning application is its dataset. A dataset is the collection of data that a machine learning application can be based on; for a neural network application, a dataset is the collection of data that the network is trained and tested. Nowadays, there are numerous datasets for visual recognition applications, with various sizes and qualities. The size of a dataset defines the number of data per class, and its quality can define many aspects of the data, such as their noise characteristics and their real correctness on their assigned labels (the creators of the dataset may wrongly label data). Some of the most popular datasets are MNIST with grayscale images of handwritten digits \cite{The-MNIST-database-of-handwritten-digits} \cite{MNIST-database-Wikipedia}, Fashion-MNIST with grayscale images of various pieces of clothing \cite{Fashion-MNIST-a-Novel-Image-Dataset-for-Benchmarking-Machine-Learning-Algorithms} \cite{Fashion-MNIST-Github}, CIFAR-10 \& CIFAR-100 with color images of 10 and 100 classes, respectively, of everyday items \cite{CIFAR-10-CIFAR-100} \cite{CIFAR-10-CIFAR-100-Wikipedia}, Microsoft COCO with color images for object recognition / detection of everyday items \cite{Microsoft-COCO-Common-Objects-in-Context} \cite{MS-COCO} and ImageNet with high resolution color images of 22000 classes of everyday items \cite{ImageNet-Official-site} \cite{ImageNet-Wikipedia}.

ImageNet is one of the biggest datasets, containing more than 14 million hand-annotated images and more than 1 million bounding boxes for those images. Since 2010, the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) is organized annually by the ImageNet project, where software programs compete in the classification of a trimmed list of one thousand non-overlapping classes. The contestant softwares have been of many different types throughout the years; however, from the ILSVRC 2012 and then on, Convolutional Neural Networks have dominated the challenge, achieving near human-like accuracy. A CNN called AlexNet \cite{ImageNet-classification-with-deep-convolutional-neural-networks} managed to achieve a top-5 error of 15.3\% in the ILSVRC 2012, where the top-5 error rate is the fraction of test data for which the correct label is not among the five most probable labels.

Many CNN architectures have been created using the aforementioned data\-sets. Some of the most important ones are being described below.

\subsection{LeNet-5}
LeNet-5 \cite{Gradient-based-learning-applied-to-document-recognition} (Figure \ref{fig:LeNet-5}), created by LeCun et al in 1998, was, at the time, a pioneering 7-layer convolutional neural network designed to recognize simple 32x32 grayscale digit images, similar to those on the MNIST dataset. It uses two convolutional layers, two pooling layers, and three fully-connected layers. However, it can only support low-resolution images and few classes, due to its low learning capacity. For higher resolution images, deeper networks are required, which was not feasible back then because of limited hardware performance. LeNet-5 was the first success for CNNs.

\begin{figure} [H]
	\centering
	\includegraphics[width=\textwidth]{Images/CNNArchitectures/LeNet-5.png}
	\decoRule
	\caption[LeNet-5 architecture]{LeNet-5 architecture}
	\label{fig:LeNet-5}
\end{figure}

\subsection{AlexNet}
AlexNet \cite{ImageNet-classification-with-deep-convolutional-neural-networks} (Figure \ref{fig:AlexNet}), created by Alex Krizhevsky et al in 2012, outperformed all prior contestants of the ILSVRC by almost twice increase in accuracy, reducing the top-5 error rate from 26.2\% to 15.3\%. It takes as input RGB 224x224 images. This is achieved by using the same types of layers as LeNet-5, but more of them, making it is deeper and with more filters per layer. Using such deep networks was made feasible due to the utilization of GPUs during the training phase, whose performance had been significantly increased. AlexNet was, also, designed to run on two GPUs simultaneously, further exploiting the CNNs' parallelism characteristics. It needed six days in time for successful training on two NVIDIA GTX 580 GPUs.

\begin{figure} [H]
	\centering
	\includegraphics[width=\textwidth]{Images/CNNArchitectures/AlexNet.png}
	\decoRule
	\caption[AlexNet architecture]{AlexNet architecture}
	\label{fig:AlexNet}
\end{figure}

Since AlexNet, most, if not all, CNNs are at least as deep, to achieve high learning capacity and accuracy.

Nowadays, AlexNet is one of the most well-known CNNs and is often used as a benchmark for various hardware solutions, due to its high complexity and number of parameters needed. In total, it uses more than 61 million parameters, which results in a size of about 250MB.

\subsection{ZFNet}
ZFNet \cite{Visualizing-and-Understanding-Convolutional-Networks} (Figure \ref{fig:ZFNet}) is a fine-tuned version of AlexNet by Matthew Zeiler and Rob Fergus, which won the ILSVRC 2013, achieving a top-5 error rate of 14.8\%.

\begin{figure} [H]
	\centering
	\includegraphics[width=\textwidth]{Images/CNNArchitectures/ZFNet.png}
	\decoRule
	\caption[ZFNet architecture]{ZFNet architecture}
	\label{fig:ZFNet}
\end{figure}

\subsection{GoogLeNet / Inception}
GoogLeNet \cite{Going-Deeper-with-Convolutions} (Figure \ref{fig:GoogLeNet}), also known as Inception v1, designed by Google, won the ILSVRC 2014 with a top-5 error rate of 6.67\%. Provided that GoogLe\-Net's top-5 error rate was near to the human level, organizers had to evaluate these results with the help of a human expert, previously trained for a few days, who achieved a top-5 error rate of 5.1\% for a single model and 3.6\% for the ensemble. This architecture, inspired by LeNet-5, consists of 22 layers but reduces the number of parameters compared to AlexNet from 61 million to only 4 million. The parameter reduction was achieved by using a special module called Inception, which is based on several very small convolutions. To further improve the accuracy, techniques such as batch normalization, image distortions, and RMSprop were used. Note that the auxiliary classifiers, Softmax0 and Softmax1, are only used during the training phase to combat the vanishing gradient problem and to provide regularization.

\begin{figure} [H]
	\centering
	\includegraphics[width=\textwidth]{Images/CNNArchitectures/GoogLeNet.png}
	\decoRule
	\caption[GoogLeNet/Inception v1 architecture]{GoogLeNet/Inception v1 architecture}
	\label{fig:GoogLeNet}
\end{figure}

\subsection{VGGNet}
VGGNet \cite{Very-Deep-Convolutional-Networks-for-Large-Scale-Image-Recognition} (Figure \ref{fig:VGGNet}), designed by Simonyan and Zisserman, was the runner-up ate the ILSVRC 2014. The original architecture consists of 16 layers, but there are other variants with more or fewer layers. In comparison to AlexNet, it uses more filters, and it was trained with 4 GPUs for up to 3 weeks. Nowadays, it is the most preferred network for image feature extraction; however, it can be very challenging, due to its 138 million parameters.

\begin{figure} [H]
	\centering
	\includegraphics[width=\textwidth]{Images/CNNArchitectures/VGGNet.png}
	\decoRule
	\caption[VGGNet architectures]{VGGNet architectures: \href{https://medium.com/coinmonks/paper-review-of-vggnet-1st-runner-up-of-ilsvlc-2014-image-classification-d02355543a11}{URL}}
	\label{fig:VGGNet}
\end{figure}

\subsection{ResNet}
ResNet \cite{Deep-Residual-Learning-for-Image-Recognition} (Figure \ref{fig:ResNet}), designed by Microsoft, won the ILSVRC 2015 with an outstanding top-5 error rate of 3.57\%, beating the human-level performance. This was achieved with the use of 152 layers. Although the layers' number may be high, its complexity is kept lower than the VGGNet due to its architecture called Residual Neural Network. It uses gated (recurrent) units, a module inspired by the recent successful elements used in RNNs, which, in a sense, skips connections. In addition, heavy batch normalizations are also used.

\begin{figure} [H]
	\centering
	\includegraphics[width=\textwidth]{Images/CNNArchitectures/ResNet.png}
	\decoRule
	\caption[ResNet architecture]{ResNet architecture - Left: the VGG-19 model (19.6 GFLOPs) as a reference. Middle: a plain network with 34 parameter layers (3.6 GFLOPs). Right: a residual network with 34 parameter layers (3.6 GFLOPs). The dotted shortcuts increase dimensions.}
	\label{fig:ResNet}
\end{figure}

\subsection{Summary}
One can observe that all the aforementioned architectures use the same types of layers found on classical CNNs. They only differ in some training techniques, their depths, and hyperparameters. In this work, AlexNet is the primary benchmark used to test the various hardware architectures, because of its simplicity and its high complexity.

\section{Deep Learning Frameworks}
There are many software frameworks available for developers to use for their deep learning applications. Their differences, apart from their syntax, are found on the amount of abstraction, portability, and their environment. Some of the most popular ones are described below.

\subsection{Keras}
Keras \cite{Keras-Official-site} \cite{Keras-Wikipedia} is a high-level library for Python, built on top of other lower-level frameworks such as TensorFlow, Theano and CNTK. While the high-level approach reduces the creation of massive deep learning models to single-line functions, it also makes the library's environment less configurable. It is best suited for learning and prototyping, with the abstraction of the mathematical operations applied.

\subsection{CAFFE}
CAFFE (Convolutional Architecture for Fast Feature Embedding) \cite{Caffe-Convolutional-Architecture-for-Fast-Feature-Embedding} \cite{Caffe-Official-site} \cite{Caffe-Wikipedia} is a deep learning framework written in C++, with a Python interface, originally developed at University of California, Berkley. It supports CNN, RCNN, LSTM, and Fully-Connected neural network models, with both CPU and GPU acceleration. Its successor, CAFFE2, also supports RNNs. CAFFE2 is now merged into PyTorch.

\subsection{PyTorch}
PyTorch \cite{PyTorch-Official-site} \cite{PyTorch-Wikipedia} developed by Facebook, is a lower-level framework based on Torch, which supports any kind of neural networks. It contains many pre-trained models, most of those in section \ref{section:CNN-Architectures}, and can also utilize GPU acceleration. It operates with a dynamically updated graph, which allows for changes to the architecture in the process. It is best suited for small projects, prototyping, and research purposes.

\subsection{TensorFlow}
TensorFlow \cite{TensorFlow-Large-Scale-Machine-Learning-on-Heterogeneous-Distributed-Systems} \cite{TensorFlow-Official-site} \cite{TensorFlow-Wikipedia} is the most popular deep learning framework nowadays. Developed by Google, it has interfaces for Python, Javascript, C++, C\#, Java, Go, and Julia. It is the most used framework for production, with support of CPU, GPU, and TPU acceleration. It can, not only, run on powerful computing clusters, but also, on mobile platforms and embedded systems. While it is a lower-level framework and needs much coding, it provides high configurability. In contrast to PyTorch, it operates with a static computation graph, which enhances its efficiency but sacrifices the ease of model modifications; for every modification, a model retrain is needed.

\section{GPU Approach}
Most of the aforementioned deep learning frameworks support GPU acceleration. GPUs can have hundreds or even thousands of more cores compared to CPUs, with even specialized ones called Tensor core for tensor operations, which makes them ideal for executing algorithms with high parallelism characteristics, such as those used in deep learning. In addition, due to the massive amounts of data that deep learning algorithms need to handle, High Bandwidth Memory (HBM) can be a perfect fit providing up to 750 GB/s compared to only 50 GB/s offered by traditional CPUs. Moreover, multiple GPUs in a single system can be utilized to further expand its parallelism capabilities.

As of right now, only NVIDIA GPUs are widely supported by most frameworks. NVIDIA's GPUs are optimized for deep learning frameworks with compatibility for Compute Unified Device Architecture Software Development Kit (CUDA SDK) \cite{NVIDIA-CUDA}, which supports many programming languages such as C and C++, increasing the GPUs usability. NVIDIA has also developed the CUDA Deep Neural Network library (cuDNN) \cite{cuDNN-Efficient-Primitives-for-Deep-Learning} \cite{NVIDIA-cuDNN}, designed to accelerate frameworks such as TensorFlow and PyTorch by providing highly-optimized implementations of routines like forward and backward convolution. Furthermore, NVIDIA's TensorRT \cite{NVIDIA-TensorRT} is an SDK for high\-ly optimized inference applications, delivering low latency and high throughput. PyTorch also supports PyCUDA \cite{NVIDIA-PyCUDA}, NVIDIA's CUDA parallel computation API for Python.

\section{Tensor Processing Unit (TPU)}
Tensor Processing Unit (TPU) \cite{In-Datacenter-Performance-Analysis-of-a-Tensor-Processing-Unit} is a custom Application Specific Integrated Circuit (ASIC), designed by Google, that accelerates the inference phase on neural networks. It is deployed to their datacenters since 2015, with the first architecture generation achieving up to 200x speedup compared to a server-class Intel Haswell CPU and up to 70x speedup compared to an NVIDIA K80 GPU. Nowadays, they have designed another two generations of its TPU (latest is TPU v3), and an edge computing solution called Coral Edge TPU \cite{Coral-Edge-TPU}, which comes in various form factors, achieving up to 4 TOPS using only 2 Watts. The first generation of TPUs was targeted for inference applications, accelerating Google's 95\% of its datacenters' neural network load, and was designed for high volume of low precision computation, with as low as 8-bit precision. However, from the second generation, TPUs support floating-point arithmetic making them ideal for training accelerators. Google also made its TPU infrastructure available to the public on its Google Compute Engine.

Every TPU v3 board is comprised of 4 chips (Figure \ref{fig:google-tpu-motherboard}), with each chip containing 2 TPU cores. Each TPU core provides 2 MXUs and is provided 16GB of HBM. Each MXU is capable of 16k Multiply-Accumulate (MAC) operation per cycle, which gives the bulk of the compute power. TPU boards are organized into pods (Figure \ref{fig:tpu-v3-pods}), with each pod containing up to 2048 TPU cores and 32TB total memory \cite{Google-Cloud-TPU}, providing a total of up to 92 PetaFLOPS of performance \cite{Tensor-Processing-Unit-Wikipedia}.

The TensorFlow framework is used to run applications on TPUs, with a reduced bfloat16 precision. Google created the bfloat16 16-bit floating-point representation standard to provide better training and model accuracy compared to IEEE half-precision representation.

In general, TPUs are worth using with very large models, needing weeks or even months of training, dominated by matrix computations, and no custom TensorFlow operations inside the main training loop. Otherwise, GPUs or ever CPUs may be more suitable and cost-effective \cite{Cloud-Tensor-Processing-Units}.

\begin{figure} [H]
	\centering
	\includegraphics[width=\textwidth]{Images/Hardware/tpu-v3-pods.png}
	\decoRule
	\caption[Google Cloud TPU v3 Pods]{Google Cloud TPU v3 Pods}
	\label{fig:tpu-v3-pods}
\end{figure}

\section{The FPGA Perspective}
\subsection{Xilinx CHaiDNN}
\subsection{Xilinx Deep Learning Processor Unit (DPU)}

\section{Thesis Approach}
