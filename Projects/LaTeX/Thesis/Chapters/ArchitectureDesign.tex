\chapter{Architecture Design}
\label{Chapter-Architecture-Design}

For this work, a platform was created capable of CNN inference on FPGA devices. This platform had to be created with flexibility and versatility in mind to be able to be transferred to other FPGA devices while being based on the Xilinx ZCU102, which was available in the lab. It should also be scalable to enable multi-FPGA implementations, and it should be extendable to enable for easy adding of new layer types and new layer accelerators. Furthermore, it should be able to run various CNN models' inference, but most importantly, it should provide easy experimentation and development of hardware accelerator architectures.

The basic building blocks of this platform consists of its volatile and non-volatile memory, and its compute engine. Figure \ref{fig:platform-block-diagram} depicts the platform's block diagram, whose functionality is explained below. It should be noted that everything described below is implemented on the aforementioned Xilinx ZCU102 Evaluation Kit.

\begin{figure} [H]
	\centering
	\includegraphics[width=\textwidth]{../Images/Platform/platform.png}
	\decoRule
	\caption[The platform's block diagram]{The platform's block diagram.}
	\label{fig:platform-block-diagram}
\end{figure}

\section{Non-Volatile Memory}
This platform's non-volatile memory serves as a storage medium for all the data that networks require for their inference, which include the initialization data (network model configurations, parameters (weights and biases), class labels), and the input data (e.g., images). An SD-card is used as the non-volatile memory for this platform (also depicted as an SD-card in Figure \ref{fig:platform-block-diagram}). However, with some software extensions, other storage devices can also be used, such as SATA hard drives or even M.2 SSDs (on other FPGA platforms, e.g., QFDB). Moreover, external storage devices can also be utilized, via the Ethernet port through local-network/Internet, or via the JTAG port to avoid copying files over and over again on the platform's primary storage device but also to enable the single-source access for multiple FPGA devices.

While SD-cards have little bandwidth than other storage devices like SATA drives and M.2 SSDs, the non-volatile memory's purpose is to initialize the platform, which is a one time cost, and feed it with input data. When the platform is on its initialization phase, the initialization data from its non-volatile memory are transferred to its volatile memory to get later used from the compute engine. Because most, if not all, networks' initialization data fit in the volatile memory, leaving a lot of empty memory space, input data are also transferred, filling as much space as possible to utilize the volatile memory's high bandwidth for input data consumption. Consequently, the SD-card's low bandwidth can be safely ignored.

If the input data are large enough to not completely fit in the volatile memory and their consumption is faster than their feed through the SD-card, a faster storage device should be used. However, in this work's experiments, this has never been the case.

\section{Volatile Memory}
The platform's volatile memory consists of two types of memory devices, the DDR4 modules found on the ZCU102 and the on-chip BRAM. The ZCU102 also provides a separate DDR4 component accessible from the FPGA's PL part, however for simplicity and generality (it is not provided on all FPGA platforms) purposes, and because it provides lower bandwidth compared to the DDR4 modules, it is not utilized on this platform.

The platform's primary memory medium is the DDR4 memory modules  (also depicted as a DDR4 module in Figure \ref{fig:platform-block-diagram}) connected to both the PS and PL parts of the device. As mentioned before, it stores and serves all the required data for the platform to run. Those data are read from various files found on the non-volatile memory, in this case, the SD-card, using the ARM cores found on the PS part, and are then stored onto the DDR.

The DDR has to feed the PL part with data in chunks because, as explained in section \ref{sec:Memory-Footprint}, the integrated BRAM cannot store the whole network's parameters and activations. Consequently, every network's layer has to know where to find and how to access every piece of data it requires, information that is found on the platform's software structures and passed to the layers using the ARM cores during the setup phase.

On this platform, there is no central BRAM component that every accelerator accesses. Instead, every accelerator should implement its own BRAM, without permitting access to others. It should be the only one to know how to manage its storage. Of course, to fulfill the accelerator's needs in bandwidth, read and write ports, and latency, it can implement its BRAM as it requires, and utilize individual registers.

\section{Compute Engine}
The compute engine consists of both the PS and PL part of the FPGA device, which includes the ARM cores and the FPGA accelerators, respectively. While the bulk of the computation is handled by the PL part, the PS part handles the more sophisticated computations, such as the input data pre-processing and the accelerator configuration and scheduling.

In this platform, a network can be run only if its layers are supported by either software or hardware implementations. Software implementations are running on the ARM cores, and hardware implementations are the FPGA accelerators. By allowing both types of implementations, this platform expands its flexibility. This can not only help the development stage of FPGA accelerators by comparing their outputs with the ones generated by the software implementation, but it can also help to fully utilize the device's resources by running parts of even whole layers on the ARM cores in parallel to the hardware accelerators. Furthermore, layers that have not yet been implemented in hardware can easily be implemented in software for experimentation purposes, such as the Depth Concat layer found on the GoogLeNet (Figure \ref{fig:GoogLeNet}).

As shown in Figure \ref{fig:platform-block-diagram}, in this work, three layer types have been implemented in hardware; the Convolutional (Conv) layer, the Max-Pooling (MaxPool) layer and the Fully-Connected (Linear) layer. In addition, multiple instances of the same layer type can also be added in the platform to enable for either parallel execution of different inferences with different inputs or even different network models, or better pipelining in a single inference. This is discussed in detail in section \ref{sec:Scheduler-Strategies}.

In this work, every accelerator implements a layer type, and, therefore, the platform assumes that every accelerator can handle and compute a whole layer type's computation. Every accelerator comes with its driver, which is integrated into the platform's software. The driver is responsible for setting up its accelerator, which includes setting the layer's hyperparameters and passing the information of where to find its parameters (weights and biases), its input data, and where to send its output data. Also, the driver handles the accelerator's interrupts, which,  in this work, only includes the accelerator completion interrupt.

The assumption that every accelerator implements a whole layer type simplifies the accelerator's driver. As a result, the driver does not have to know its accelerator's inner workings and architecture, except for its aforementioned settings, treating it as a black box with inputs and outputs. However, those settings are the same for every implementation of the same layer type; hence, experimenting with different architectures is made easy without developing a different driver for every single architecture. The driver can be reused on the same layer type accelerators, letting the engineer focus their efforts on the accelerator's architecture. Of course, every different layer type needs different settings; therefore, different drivers.

It is worth mentioning that every accelerator has an AXI4-Lite \cite{UG1037-Vivado-Design-Suite-AXI-Reference-Guide} slave port through which it can get configured. All accelerators' AXI4-Lite slave ports are connected to a Xilinx AXI SmartConnect IP \cite{PG247-SmartConnect-Product-Guide}, onto its master ports. The AXI SmartConnect, a replacement of the AXI Interconnect IP, acts as a router, providing a single slave port that is finally connected to the Zynq's (ARM cores) master port. AXI SmartConnect automatically detects the port's protocol, in this case, AXI4-Lite, and adjusts its ports accordingly. This creates a hardware connection so that the software can access and configure the platform's accelerators. Details on the AXI protocol are given in section \ref{sec:AMBA-AXI4-Interface-Protocol}.

\section{I/O}
\label{sec:IO}
As explained in previous sections, the I/O bandwidth provided to any hardware platform plays an essential role in its performance due to the CNNs' high bandwidth requirements. Hence, the platform should implement a network for its accelerators to enable the communication between them, the PS part, the DDR, and each other. There are three ways for high bandwidth communication, which are discussed below.
\begin{itemize}
	\item \textbf{Memory-Mapped I/O (MMIO):} In this method, the PS communicates with its PL part's accelerators using a global address space for both its main memory (DDR) and the accelerators' I/O extensions, mapping every memory component, such as the DDR, BRAM, and registers, onto its own address range. While it is straightforward to implement, it can create a bottleneck on random memory accesses onto the DDR, with each request costing up to 50 clock cycles for the DDR's initialization. Fortunately, this is not the case with burst accesses, which request only once a range of addresses so that there is only one initialization cost for a big transfer of data.
	\item \textbf{Streaming (AXI4-Stream):} Using streaming interfaces, such as the AXI4-Stream, continuous communication can be established between components in a FIFO manner. Every streaming connection creates a channel between the two components with a predefined FIFO size, with a producer-consumer behavior. Communication between hardware accelerators and the DDR can be established using the Xilinx AXI DMA IP \cite{PG021-AXI-DMA-Product-Guide}, which hides the DDR's initialization costs. However, AXI DMA requires knowledge of the data flow, and its function is instructed by the PS part for every transfer, increasing the system's complexity.
	\item \textbf{BRAM:} This method is an MMIO variation, which uses BRAM IPs to store the required data on-chip, taking advantage of the BRAM's high bandwidth. Data are transferred from the DDR to the BRAM in bursts using a Xilinx CDMA IP \cite{PG034-AXI-Central-Direct-Memory-Access-Product-Guide} connected to a Xilinx BRAM Controller IP \cite{PG078-AXI-BRAM-Controller-Product-Guide} and its Xilinx Block Memory Generator IP \cite{PG058-Block-Memory-Generator-Product-Guide}. Though, a major constraint is that the data have to be small enough to fit into the chip's BRAM, which is in the order of a few MBs in size.
\end{itemize}

As explained in section \ref{sec:Memory-Footprint}, the integrated BRAM cannot store the parameters for most layers. There is the case that parameters can be transferred in chunks to the central Block Memory IP; however, this creates further complexity and bandwidth bottleneck due to the low number of read and write ports. Therefore, the BRAM method described above cannot be used in this platform.

Hence, the question that arises is which method is more suitable for this platform, the MMIO or the Streaming one. To answer it, a test system was created, integrating a simple IP, created using Vivado HLS, that adds a specific value to its input data, and then returns its output data. The input data are generated on the PS part and are stored on the system's DDR. The IP's output is returned to the PS part and stored on its DDR. Then the PS validates the outputs to ensure their correctness. The IP's input and output ports are implemented both as streams and as MMIO.

The test measures the average number of clock cycles it takes for both implementations to process 40MB of data in chunks of 40kB. The chunk's size was selected greedily, because most, if not all, accelerators' requests need to be at least 40kB. The time measurement includes the input data transfer from the DDR to the IP's BRAM, the input data processing, and the output data transfer from the IP's BRAM to the system's DDR. Both implementations were tested with 32 and 128 input and output ports bit-width. The test's results are shown on table \ref{tab:MMIO-vs-Stream}.

\begin{table}[H]
	\caption[MMIO vs Stream]{MMIO vs Stream: Processing 40MB data of 40KB bursts, showing a slight advantage over the MMIO method.}
	\label{tab:MMIO-vs-Stream}
	\centering
	\begin{tabular}{lll}
		\toprule
		\textbf{Port bit-width} & \textbf{MMIO avg. cycles} & \textbf{Streaming avg. cycles}\\
		\midrule
			32-bit 	& 62700922 & 65611580\\
			128-bit & 15761270 & 16201797\\
		\bottomrule
	\end{tabular}
\end{table}

Both implementations for both port bit-widths show similar results, with a slight advantage over the MMIO implementation. Selecting MMIO for the primary method of data feeding the accelerators benefits the platform not only in terms of bandwidth, according to the slight advantage depicted on table \ref{tab:MMIO-vs-Stream}, but also in terms of simplicity of both software and hardware implementation and efficiency of hardware resources.

Consequently, the connection between the accelerators and the PS part and its DDR is established using the MMIO method, as shown on the platform's block diagram (Figure \ref{fig:platform-block-diagram}). Every accelerator has an AXI4-Full \cite{UG1037-Vivado-Design-Suite-AXI-Reference-Guide} master port, which is connected to a Xilinx AXI SmartConnect IP, onto its slave ports. The SmartConnect's master port then gets connected onto the Zynq's slave port. Communication to the DDR is established via the Zynq's slave port, and an integrated DMA found on the PS part.

However, the streaming method cannot be dismissed entirely, as it is perfect for communication between accelerators. Passing a layer's activations to its next layer, in terms of hardware means passing the accelerator's output data to the next accelerator as input data. Implementing this data transfer in a streaming manner avoids transferring data from the FPGA to the DDR and then transferring them back to the FPGA using the MMIO method, which can be a significant bottleneck. As a result, every accelerator can optionally have additional input stream and output stream ports. The accelerators driver configures it to either use the MMIO or the streaming method.

It is vital that every accelerator implements at least the MMIO method, to avoid running into deadlocks. For example, let there be a system with one instance of a layer type's accelerator, and the accelerator's output stream needs to get connected back to the same accelerator's input stream. A deadlock occurs when the output stream is full while the accelerator has not finished its execution. In this case, the accelerator hangs, waiting for the stream to accept more data, while the stream hangs, waiting for its data to get consumed. This problem can easily be tackled by simply sending the accelerator's outputs to the DDR using the MMIO method, and when its execution is complete, it can then read them back again from the DDR.

A Xilinx AXI4-Stream Switch IP \cite{PG085-AXI4-Stream-Infrastructure-IP-Suite-Product-Guide} is used to create a star topology \cite{Network-Topology-Wikipedia} stream network, connecting every accelerator's stream ports, as shown on the platform's block diagram (Figure \ref{fig:platform-block-diagram}). Every connection is assigned an address in the AXI4-Stream Switch IP before synthesis so that the TDEST signal of the AXI4-Stream protocol can be set accordingly by the sender accelerator. The AXI4-Stream Switch IP was preferred to the AXI4-Stream Interconnect IP because the latter is the same as the Switch, but it also allows connections of streams with different characteristics in the cost of some hardware resources. However, all accelerators are implemented with the same stream characteristics, so the Interconnect's use is redundant.

\subsection{AMBA AXI4 Interface Protocol}
\label{sec:AMBA-AXI4-Interface-Protocol}
The AMBA AXI4 (Advanced eXtensible Interface 4) \cite{UG1037-Vivado-Design-Suite-AXI-Reference-Guide} is the AMBA interface specification from ARM, which is integrated into the Xilinx Design Suite tools to offer a single standard interface and simplify the IP integration. All Xilinx IPs that require any configuration or large amounts of data implement at least one of the AXI4's variation. The AXI4 protocol has three variations for different use cases.
\begin{itemize}
	\item \textbf{AXI4-Full:} Used for high-performance memory-mapped applications, supporting bursts of up to 256 beats.
	\item \textbf{AXI4-Lite:} The simplest of all three variations, with the least hardware resources requirements. It is used for low-throughput memory-mapped applications, usually for accessing control registers, with every transaction having a burst length of one
	\item \textbf{AXI4-Stream:} Used for high-speed streaming unidirectional data transfers from master to slave, supporting multiple data streams using the same set of shared wires, and multiple data widths withing the same interconnect.
\end{itemize}

This platform makes usage of all three variations for different purposes, as shown on its block diagram (Figure \ref{fig:platform-block-diagram}). The AXI4-Lite is used for configuring the accelerators; the AXI4-Full is used to implement the accelerators' MMIO, as described in section \ref{sec:IO}, and the AXI4-Stream is used to create the star topology network between the accelerators.

\section{Software}
The platform's software was developed using the Xilinx Vitis IDE, previously known as Xilinx SDK. Xilinx Vitis provides the necessary compilation toolchain, and the ability to program the FPGA and run software on the ARM cores. This platform cannot function without its software. It should be clarified that everything that is considered a software part runs on the device's processor (ARM cores). The software consists of the accelerators' drivers, the scheduler, the application logic, and the user interface.

The drivers are responsible for the communication between the software and hardware of this platform. They are used to handle the initialization, configuration, and use of each hardware component. They also handle the accelerators' various interrupt events, affecting the software's execution. Every accelerator's driver has to implement several functions with specific functionality and naming scheme, creating an abstraction layer between the drivers and the rest of the software. This makes integrating a new accelerator into the platform easier because the other software parts "know" how to use the new driver, expanding the platform's modularity. Moreover, it is essential that the interrupt service routines are kept as small as possible to avoid missing any other interrupt events while executing them.

The application logic is responsible for configuring the platform depending on the user's input. It handles the parsing of the network model configuration files, the loading of the network's initialization data (parameters and labels), the input data preprocessing, and the accelerator execution according to the scheduler's instructions. It is designed with data arbitration in mind, in order to easily experiment with different data types used in the network's parameters and activations. The application logic's data type is the same with the one used in the accelerators' architecture, and when set, it propagates to the whole software.

For the sake of simplicity, a command-line interface implements the user interface. Most FPGA platforms have a UART port that can be used to print messages, accept user input, and debugging. Therefore, this platform utilizes the UART port for its user input requirements. From the application's menus, the user can select several functions, among which there is a self-test routine that tests the platform's integrity, a network model selection with its corresponding parameters, an image count selection, and an option to run both software and hardware implementations of every layer for output data checking. While not implemented, the user interface can be expanded to a graphical user interface using a standalone program or server, running on a host PC that reads and writes the aforementioned UART port.

The scheduler is the last but not least, part of the platform's software. It is responsible for scheduling all the necessary tasks for the network's inference to execute. A task can be executing a layer or a group of layers either in software or using the hardware accelerators. The scheduler is also responsible for scheduling the input and output methods of each accelerator (MMIO and Streaming), the input source and output destination. Per platform implementation, there might be needed a different scheduler strategy. For example, there is a different strategy for a platform with only one instance per accelerator type, and a different one when there are multiple, or when the accelerators' execution can be pipelined. For more information on the platform's scheduler strategies see section \ref{sec:Scheduler-Strategies}

A simplified flowchart of the platform's software execution can be seen in figure \ref{fig:platform-flowchart}. On system boot-up, the drivers are discovering and initializing all peripheral devices, such as the SD-card and the UART ports. Afterward, they discover and initialize every accelerator existing in the FPGA's PL part. During this step, the interrupt handlers are also getting set. Next, the user interface asks the user for which network from the available it should run its inference and all the aforementioned running options. Given the user's input, the application logic reads the network model file, parses it, and loads it to the platform's memory. It also loads the network's parameters, labels, and input data. Then, for each input data, the scheduler adds the tasks into a list to be run whenever possible. Then, for every task, the corresponding accelerator is set up and triggered to run using its drivers. The platform finishes when there are no more tasks to be run and input data to get processed.

\begin{figure} [H]
	\centering
	\includegraphics[width=\textwidth]{../Images/Platform/PlatformFlowchart.png}
	\decoRule
	\caption[The platform's flowchart]{The platform's flowchart.}
	\label{fig:platform-flowchart}
\end{figure}

\section{Scheduler Strategies}
\label{sec:Scheduler-Strategies}
Scheduling can play a significant role in the platform's performance. As mentioned in the previous section, the scheduling strategy is dependent on the platform's implementation and goals. The number of same-type accelerator instances and whether the platform is throughput or latency optimized, all affect the strategy selection. To study and demonstrate the various strategies, a MATLAB model of a CNN network's inference execution was created that can compute the execution characteristics of any CNN network model. This work's main CNN network is AlexNet, on which all experiments below are based.

The MATLAB model creates a timed schedule of a CNN inference depending on its hyperparameters. It computes the number of clock cycles a layer requires for its full execution, and depending on the strategy selected, it places the layer's execution start and finish timestamps. The clock cycles required for every full execution is equal to the sum of the clock cycles needed per assembly instruction to be executed. The number of clock cycles required for every assembly instruction can be set on the MATLAB model's parameters. However, for simplicity, all instructions are considered to run in a single clock cycle.

The following figures show the starting clock cycle, the ending clock cycle, and the duration of every layer, wherever there are colored boxes. Every colored box has its label, defining the layer that it represents. The label is coded as the layer's type and its serial number; Conv for convolutional layers, MaxPool for max-pooling layers, and Linear for fully-connected layers. It should be noted that the ReLU activation function is embedded into the convolutional and fully-connected layers, as shown on algorithms \ref{alg:Convolution-Layer-with-ReLU} and \ref{alg:Fully-Connected-Layer-with-ReLU}.

\subsection{Serial Strategy}
A baseline schedule was created using a serial execution strategy, as shown in figure \ref{fig:serial-execution}. In this strategy, every layer starts when the previous layer has finished generating all its outputs. This strategy can be used when there is only one accelerator instance per layer type, and the accelerators do not support layer pipelining. It is also an excellent strategy for debugging and validating the platform and its accelerators because of its simplicity.

\begin{figure} [H]
	\centering
	\includegraphics[width=\textwidth]{../Images/Scheduling/Serial.png}
	\decoRule
	\caption[AlexNet serial execution]{AlexNet serial execution: Convolution layers consume 90\% of total clock cycles needed for a full inference.}
	\label{fig:serial-execution}
\end{figure}

It can be observed that using the serial execution strategy, around 90\% of the total clock cycles is consumed from the convolutional layers, and almost 0\% is consumed from the max-pooling layers. Hence, according to Amdahl's law, see section \ref{sec:Amdahls-Law}, the convolutional layers should get the most hardware resources for their accelerators to create as much parallelism as possible, while fully-connected layers come next, and max-pooling layers come last.

\subsection{Layer-Pipelining Strategy}
Another scheduling strategy is applying pipelining within the layers. In this strategy, a layer is fed with input data as soon as the previous layer generates a single output. This hides the next layer's latency as much as possible by computing outputs in parallel with the computation of its next-to-be-processed inputs. A schedule using the layer-pipelined strategy is shown in figure \ref{fig:layer-pipelined-execution}. To produce such a schedule using this strategy, it is considered to exist as many accelerators per layer type as needed. In this schedule, there are needed five instances of a convolutional accelerator,  three instances of a max-pooling accelerator, and three instances of a fully-connected accelerator.

\begin{figure} [H]
	\centering
	\includegraphics[width=\textwidth]{../Images/Scheduling/Pipelined-1x.png}
	\decoRule
	\caption[AlexNet layer-pipelined execution]{AlexNet layer-pipelined execution: There is a speedup of almost 3 times compared to the serial strategy.}
	\label{fig:layer-pipelined-execution}
\end{figure}

The first four layers, Conv1, MaxPool1, Conv2, and MaxPool2 seem to start their execution immediately; however, this is not the case. In fact, they all start on different timestamps, each later from its previous one, but it just cannot be shown in figure \ref{fig:layer-pipelined-execution} due to the x-axis' scale. Conv1, MaxPool1, and Conv2 generate their first outputs in a few clock cycles compared to the x-axis scale. The same artifact appears on the finishing timestamps of the first two layers, Conv1 and MaxPool1, and the next six layers, Conv2 to MaxPool3.

From figure \ref{fig:layer-pipelined-execution}, it can be seen that the whole execution has significantly been reduced compared to the serial strategy shown in figure \ref{fig:serial-execution}, speeding it up almost three times.

However, to use the layer-pipelined strategy, the accelerators have to support it. This means that they need to process their inputs as soon as possible to generate their outputs. Because convolutional and max-pooling layers process their inputs in the 3D and 2D space, they have different implementation requirements to support layer-pipelining, compared to the fully-connected layers that compute their inputs in the 1D space.

Convolutional and max-pooling layers need to process their inputs in a way that they can generate outputs in a specific order. The optimal output order is the one that the next layer wants its inputs to be in, to also produce useful outputs for its next layer. The convolutional and the max-pooling layers produce outputs using cubes or squares of inputs, respectively, starting from the top left input of the grid. Those inputs are created from the previous layer, which also generated them using cubes or squares of inputs. Hence, the deeper the layer, the bigger the required cube or square of the first layer's inputs. Consequently, the optimal order of generating outputs starts from the top left and moving downwards and right in zones, always creating bigger and bigger cubes or squares. Figure \ref{fig:output-generation-order} depicts the order a convolutional or a max-pooling layer generates its outputs. It should be noted that the convolutional layers generate their outputs for all of their output channels, and then they move onto the next output.

\begin{figure} [H]
	\centering
	\includegraphics[width=\textwidth]{../Images/Scheduling/Conv5-output-creation-time.png}
	\decoRule
	\caption[Convolutional and Max-Pooling layer output order for layer-pipelining]{Convolutional and Max-Pooling layer output order for layer-pipelining: Outputs colored in blue are generated before the red ones.}
	\label{fig:output-generation-order}
\end{figure}

In figure \ref{fig:output-generation-order}, every pixel is an output of a convolutional or max-pooling layer, and it is color-coded concerning the time it is generated, with blue and red colors representing the start and end, respectively, of the generation time.

However, layer-pipelining for convolutional and max-pooling layers significantly increased the implementation complexity of their accelerators. Also, caching weights and inputs in the accelerator's BRAM and registers can become a major obstacle due to their size and access patterns. Figure \ref{fig:Conv5-pixel-frequency} shows the input usage frequency for a convolutional or a max-pooling layer with a stride of one.

\begin{figure} [H]
	\centering
	\includegraphics[width=\textwidth]{../Images/Scheduling/Conv5-pixel-frequency.png}
	\decoRule
	\caption[Convolutional and Max-Pooling layer input pixel usage frequency using a stride of one]{Convolutional and Max-Pooling layer input pixel usage frequency a stride of one: Blue inputs are rarely used, while red ones are used frequently.}
	\label{fig:Conv5-pixel-frequency}
\end{figure}

Figure \ref{fig:Conv1-pixel-frequency} depicts the input usage frequency for a convolutional or max-pooling layer with a stride of four, showing even more complex access patterns.

\begin{figure} [H]
	\centering
	\includegraphics[width=\textwidth]{../Images/Scheduling/Conv1-pixel-frequency.png}
	\decoRule
	\caption[Convolutional and Max-Pooling layer input pixel usage frequency using a stride of four]{Convolutional and Max-Pooling layer input pixel usage frequency using a stride of four: Blue inputs are rarely used, while red ones are used frequently.}
	\label{fig:Conv1-pixel-frequency}
\end{figure}

On the other hand, fully-connected layers do not require a specific order for their inputs. However, they need to store partial results for their outputs. Whenever an input is given to the fully-connected layer, it adds it to the partial result of each output after multiplying it with its corresponding weight per output. This creates higher memory requirements for the implementation of the fully-connected accelerator.

Unfortunately, due to the layer-pipelining's significantly increased complexity, this work does not implement it on its accelerators, and it is only presented for completeness and ideas for future work.

\subsection{Multi-Inference Strategy}
When multiple accelerators per layer type can be placed into the FPGA device's PL part, they can be utilized simultaneously by running multiple inferences in parallel. The multi-inference strategy schedules two or more images for inference, increasing the platform's overall throughput.

\subsection{Image-Pipelining Strategy}
Similar to the multi-inference strategy, when there are multiple accelerators per layer type, multiple images can be fed into the network in a pipelined manner. Every accelerator instance represents a single layer of the network. Hence, the first image can be fed into the first layer. Afterward, when the first layer has generated its outputs, they are fed to the second layer, and the first layer is fed with the second image. This continues for all images to be inferenced. Therefore, when the pipeline is full, all accelerators process different input images from each other. This strategy can decrease the platform's inference latency, and, potentially, the platform's throughput.

\section{Amdahl's Law}
\label{sec:Amdahls-Law}
Amdahl's law \cite{Improvements-in-Multiprocessor-System-Design} is a formula that calculates the theoretical speedup in latency of the execution of a fixed workload task, when the system's resources are improved or increased. While speedup was firstly used on parallel processing, it can also be used after any resource enhancement.

Latency is the time required for a system to compute a single task and is defines as:
\begin{equation}
	\label{eqn:latency}
	\begin{split}
		Latency = \frac{1}{v} = \frac{T}{W},\\
		\mbox{v: the task's execution speed},\\
		\mbox{T: the task's execution time},\\
		\mbox{W: the task's execution workload}\\
	\end{split}
\end{equation}

Throughput is the maximum processing rate of a specific task and is defined as:
\begin{equation}
	\label{eqn:throughput}
	\begin{split}
		Throughput = r * v * A = \frac{r * A * W}{T} = \frac{r * A}{L},\\
		\mbox{r: the execution density},\\
		\mbox{A: the execution capacity}\\
	\end{split}
\end{equation}

The speedup is defined for both latency and throughput, as shown in the equations below:
\begin{equation}
	\label{eqn:latency-speedup}
	\begin{split}
		S_{Latency} = \frac{L_1}{L_2} = \frac{T_1 * W_2}{T_2 * W_1} = \frac{1}{(1 - p) + \frac{p}{s}},\\
		\mbox{p: the task's portion that benefits from the resource enhancements},\\
		\mbox{s: the speedup of the task's portion that benefits from the resource enhancements}\\
	\end{split}
\end{equation}
\begin{equation}
	\label{eqn:throughput-speedup}
	S_{Throughput} = \frac{Throughput_2}{Throughput_1}\\
\end{equation}

The maximum theoretical speedup can also be defined as:
\begin{equation}
	\label{eqn:max-speedup}
	MaxSpeedup = \lim_{s \to \infinity} S_{Latency} = \frac{1}{1 - p}\\
\end{equation}

\section{Platform Accelerator Architectures}
\label{sec:Platform-Accelerator-Architectures}
In this work, three different accelerators have been developed, one per layer type used in most CNNs. Those are the convolution, the max-pooling, and the fully-connected accelerators. Every accelerator has two variations; a simple one that only processes a single input similar to serial execution for testing purposes, and a performance-oriented one for production/system deployment. Some of the accelerators' components are being described separately to increase the readability of their architecture diagrams.

\subsection{Convolution Accelerator}
The convolution accelerator is the most complex, requiring lots of BRAM slices and clock cycles to complete. Figure \ref{fig:conv-core-serial} depicts the simple version of the convolution accelerator's architecture. While it is not meant to be used in production, it is great for testing purposes and validating the platform's and accelerator's subsystems.

\begin{figure} [H]
	\centering
	\includegraphics[width=\textwidth]{../Images/Platform/Conv_core_serial.png}
	\decoRule
	\caption[Convolutional layer serial accelerator]{Convolutional layer serial accelerator.}
	\label{fig:conv-core-serial}
\end{figure}

The convolution accelerator reaches the platform's DDR memory for its parameters through an AXI4-Full port, while its input data are given either through the same AXI4-Full port or through a dedicated AXI4-Stream port. The input data source is selected using a \emph{MMIO / Stream} component, as instructed by the accelerator's \emph{controller} component, and the data are then stored into a BRAM instance for later use. The layer's weights are stored kernel by kernel onto another BRAM instance. This way, the BRAM requirements are kept low, compared to storing the whole layer's weights, which might also be impossible, due to the BRAM's limited size. Because every kernel is convoluted with the entire input, every weight in that kernel and every input pixel is accessed multiple times. Hence, the inputs and weights BRAM instances are used as caches. It is also worth noting that each BRAM instance implements only one read and one write port because only one input and one weight is accessed every clock cycle.

The computation starts when the input data and weights are stored in their corresponding BRAM instances. Firstly, the \emph{accumulator} component, whose architecture is described below, is initialized by the kernel's bias. Afterward, the controller asks every BRAM instance for specific data to be sent to the \emph{multiplier} component, whose results are then sent to the \emph{accumulator}. This operation continues with the controller selecting the appropriate data for a correct convolution until a single output is ready. Then, it is sent from the \emph{accumulator} to the \emph{ReLU} component, whose architecture is described below. The \emph{controller} then instructs the \emph{ReLU} component to either apply its activation function or pass it through. Lastly, the \emph{ReLU} component sends its output to another \emph{MMIO / Stream} component, which either sends it back to the platform's DDR or a dedicated AXI4-Stream port, also instructed by the \emph{controller}. This procedure continues until the full convolution operation is completed, processing the entire input. The \emph{controller's} configuration is handled by an AXI4-Lite port, accessed by the PS part.

The \emph{accumulator} component, shown in figure \ref{fig:accumulator-component}, is a simple accumulator, which has an input port that gets added with the register's output, used to store the partial result. The adder's output is connected back to the register's input port and the \emph{accumulator} component's output port.

\begin{figure} [H]
	\centering
	\includegraphics[scale=0.4]{../Images/Platform/Accumulator_component.png}
	\decoRule
	\caption[Accumulator component]{Accumulator component.}
	\label{fig:accumulator-component}
\end{figure}

The \emph{ReLU} component, shown in figure \ref{fig:relu-component}, applies the ReLU activation function (see section \ref{sec:Activation-Function}) to the component's input, when the enable port is set, otherwise it passes through the input to the output port. It consists of a simple comparator and two multiplexers.

\begin{figure} [H]
	\centering
	\includegraphics[scale=0.4]{../Images/Platform/ReLU_component.png}
	\decoRule
	\caption[ReLU component]{ReLU component.}
	\label{fig:relu-component}
\end{figure}

Figure \ref{fig:conv-core-row-parallel} depicts the high performance version of the convolution accelerator's architecture. It is very similar to the simple version, with the main difference being the number of inputs that can be processed in parallel. This version uses a \emph{multiplier} array, an adder tree, and multiple read and write ports on the input and weights BRAM instances. In this version, the multiplier array is fed with data using all BRAM read ports to feed all of its \emph{multiplier} components in parallel, and then it passes its results to the adder tree in order to create a single partial output. The data fed to the multiplier array consist of a single kernel row of weights and inputs. Then, the partial output is fed to the \emph{accumulator}, and a new row of data is given to the multiplier array. The rows come in the order of top to bottom row of a single channel, and then the next channel follows. This process continues until every row, and channel of a specific kernel is convoluted with the corresponding input area. Afterward, the \emph{accumulator} passes its output to the \emph{ReLU} component, and the process continues similar to the accelerator's simple version.

\begin{figure} [H]
	\centering
	\includegraphics[width=\textwidth]{../Images/Platform/Conv_core_row_parallel.png}
	\decoRule
	\caption[Convolutional layer kernel-row-parallel accelerator]{Convolutional layer kernel-row-parallel accelerator.}
	\label{fig:conv-core-row-parallel}
\end{figure}

The number of \emph{multiplier} components is set to be at least the maximum kernel size of every convolutional layer found in the network to be able to process its whole row at once. Granted that the convolution core also has to support the smaller kernel sizes found on the network's convolutional layers, there are structures that feed the excess \emph{multiplier} components with zeros to not affect the computation's result.

\subsection{Max-Pooling Accelerator}
The max-pooling accelerator is the lightest, requiring little BRAM and external I/O. Figure \ref{fig:max-pool-core-serial} depicts the simple version of the accelerator's architecture. Similar to the convolution accelerator, it is only targeted for testing and validation purposes. However, it might also be suitable for production to save resources due to its simplicity. Besides, as shown in the serial scheduling (Figure \ref{fig:serial-execution}), max-pooling layers contribute almost nothing to the overall time required for a complete inference, allowing for slower architectures, saving resources.

\begin{figure} [H]
	\centering
	\includegraphics[width=\textwidth]{../Images/Platform/MaxPool_core_serial.png}
	\decoRule
	\caption[Max-Pooling layer serial accelerator]{Max-Pooling layer serial accelerator.}
	\label{fig:max-pool-core-serial}
\end{figure}

The max-pooling accelerator's I/O is handled by two \emph{MMIO / Stream} components, similar to the convolutional accelerator. The BRAM instance, using a single read and a single write port, loads a single channel from the whole input. It then feeds the \emph{max} component, whose architecture is described below, as instructed by the accelerator's \emph{controller}. The \emph{max} component's output is fed to the \emph{register}, which feeds it back to the \emph{max} component to process the next input. In other words, this structure finds the maximum value in a given array of input data. This process continues until a whole kernel of input data is processed, generating a single output, which is then sent to the output \emph{MMIO / Stream} component. Afterward, the next part of the input is processed, and when the whole channel is completed, the next one gets loaded to the BRAM instance. The accelerator finishes when all of the input channels are processed.

The \emph{max} component, shown in figure \ref{fig:max-component}, outputs the maximum input of the two it is given. It comprises only two components, a comparator, and a multiplexer.

\begin{figure} [H]
	\centering
	\includegraphics[scale=0.4]{../Images/Platform/Max_component.png}
	\decoRule
	\caption[Max component]{Max component.}
	\label{fig:max-component}
\end{figure}

The max-pooling accelerator's high performance version architecture is depicted in figure \ref{fig:max-pool-core-kernel-parallel}. The difference between the simple architecture lies upon the BRAM instance's read ports number and the \emph{max tree} component, whose architecture is described below. Using multiple read ports, multiple inputs can be inserted to the \emph{max tree} component in a single clock cycle, which in turn it outputs the max value of its given input. This architecture processes the whole kernel, generating a single output in every iteration. On every iteration, the kernel moves onto the given input, until the channel is fully processed. Then the next channel gets loaded onto the BRAM instance, and the process continues.

\begin{figure} [H]
	\centering
	\includegraphics[width=\textwidth]{../Images/Platform/MaxPool_core_kernel_parallel.png}
	\decoRule
	\caption[Max-Pooling layer kernel-parallel accelerator]{Max-Pooling layer kernel-parallel accelerator.}
	\label{fig:max-pool-core-kernel-parallel}
\end{figure}

Like the convolution accelerator, the max-pooling accelerator must support all kernel sizes found in the network's max-pooling layer. Hence, the BRAM read ports number and the \emph{max tree} component's input ports number are set to be the network's maximum max-pooling kernel size. When a layer with a smaller kernel size is processed, the \emph{max tree} component's excess inputs are fed with the minimum possible value not to affect the computation.

The \emph{max tree} component, shown in figure \ref{fig:max-tree-component}, is a tree of \emph{max} components, capable of finding the max value from the given input. The kernel's 2D matrix is flattened into a 1D array fed into the \emph{max tree} component to find the input area's maximum value.

\begin{figure} [H]
	\centering
	\includegraphics[width=\textwidth]{../Images/Platform/MaxTree_component.png}
	\decoRule
	\caption[MaxTree component]{MaxTree component.}
	\label{fig:max-tree-component}
\end{figure}

\subsection{Fully-Connected Accelerator}
Unfortunately, the fully-connected accelerator is the most limited in terms of parallelism capabilities since the fully-connected layer's weights are used only once, and therefore, they cannot be cached into an in-accelerator memory instance (BRAM or registers). Only the input data are used multiple times, so storing them into a BRAM instance can help avoid I/O with the platform's DDR.

Figure \ref{fig:linear-core-serial} shows the simple version of the fully-connected accelerator's architecture. Like the other accelerators, the input and output data are handled, as instructed by the accelerator's \emph{controller}, using two \emph{MMIO / Stream} components, connected to their dedicated input and output AXI4-Stream ports and the AXI4-Full port. Firstly, the input data are loaded onto the BRAM instance, through the input \emph{MMIO / Stream} component. Afterward, the \emph{accumulator} component is initialized with the appropriate bias, read directly from the platform's DDR through the accelerator's AXI4-Full port. Then, the \emph{controller} instructs the BRAM instance to sequentially feed the \emph{multiplier} with a single input datum on every iteration. It also instructs the DDR to feed the \emph{multiplier}, through the AXI4-Full port, with the corresponding weight on every iteration. The \emph{multiplier} component's output is then forwarded to the \emph{accumulator}. When all weights and inputs are multiplied and accumulated, a single output is generated. It is then sent from the \emph{accumulator} directly to the \emph{ReLU} component, which applies its ReLU activation function if instructed so by the \emph{controller}. Lastly, the \emph{ReLU} component's result is sent to the output \emph{MMIO / Stream} component to write it back to the DDR or to send it to the next accelerator using its dedicated output stream port. The accelerator finishes when all inputs and all weights have been processed, generating the total output.

\begin{figure} [H]
	\centering
	\includegraphics[width=\textwidth]{../Images/Platform/Linear_core_serial.png}
	\decoRule
	\caption[Fully-Connected layer serial accelerator]{Fully-Connected layer serial accelerator.}
	\label{fig:linear-core-serial}
\end{figure}

Although this version is relatively simple, it uses a lot of BRAM only to access a single input, sequentially. Hence, the BRAM instance might be overkill for this use, wasting resources. Although, it should be noted that this simple version of the accelerator's architecture is only targeted for testing and validation purposes.

A more optimized architecture can be seen in figure \ref{fig:linear-core-partial-outputs}, where a multiplier array and an adder tree are utilized, the input BRAM instance is replaced with simple registers, and the weights are given by splitting the 128-bit wide AXI4-Full port. In a sense, the accelerator processes its input data in parts creating partial outputs on each iteration.

\begin{figure} [H]
	\centering
	\includegraphics[width=\textwidth]{../Images/Platform/Linear_core_partial_outputs.png}
	\decoRule
	\caption[Fully-Connected layer accelerator with partial outputs]{Fully-Connected layer accelerator with partial outputs.}
	\label{fig:linear-core-partial-outputs}
\end{figure}

Like the simple version, the input data are handled by the input \emph{MMIO / Stream} component; however, they are then written in parts onto the \emph{input registers}. Every register has its own read and write ports so that multiple inputs can be sent to the multiplier array in parallel. Furthermore, the weights are still read from the DDR through the accelerator's AXI4-Full port, taking advantage of the port's width. For example, if a single weight is a 32-bit float and the port is 128-bit wide, every 128-bit word can include four weights. Hence, the size of the multiplier array depends on the weight's data type. Also, there is no need for more input registers than the number of weights that can fit into a single 128-bit word, so the input data are saved and processed in parts throughout the whole output. The process starts by initializing the output BRAM instance with the biases. The output BRAM instance is used to store the partial outputs; therefore, every cell is initialized with a single bias, corresponding to the output it represents. Starting the computation, the first input part is multiplied with its corresponding weights, and then the multiplication's results are sent to the added tree, generating a partial output stored into its BRAM cell. Afterward, the same input part is multiplied with the next set of weights, and a new partial output is generated and stored. When the first input part has been processed throughout all the partial outputs, the second part takes its place, and the process continues, adding the newly created partial outputs to the already existing ones in the BRAM's cells. The process continues until the whole input has been processed, and the full outputs are generated. Lastly, the full outputs pass through the \emph{ReLU} component and are then sent to the \emph{MMIO / Stream} component.
